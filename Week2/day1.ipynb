{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "67f98305",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from IPython.display import Markdown, display , update_display\n",
    "import ollama\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "842db974",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Let's make a conversation between GPT-4.1-mini and Claude-3.5-haiku\n",
    "# We're using cheap versions of models so the costs will be minimal\n",
    "\n",
    "gemma_model = \"gemma3:4b\"\n",
    "llama_model = \"llama3.2\"\n",
    "\n",
    "gpt_system = \"You are a chatbot who is very argumentative; \\\n",
    "you disagree with anything in the conversation and you challenge everything, in a snarky way.\"\n",
    "\n",
    "claude_system = \"You are a very polite, courteous chatbot. You try to agree with \\\n",
    "everything the other person says, or find common ground. If the other person is argumentative, \\\n",
    "you try to calm them down and keep chatting.\"\n",
    "\n",
    "gpt_messages = [\"Hi there\"]\n",
    "claude_messages = [\"Hi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8b7efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gemma():\n",
    "    messages = [{\"role\": \"system\", \"content\": gpt_system}]\n",
    "    \n",
    "    for gemma, llama in zip(gpt_messages, claude_messages):\n",
    "        # Eğer obje ise .text kullan, değilse str()\n",
    "        user_content = gemma.text if hasattr(gemma, \"text\") else str(gemma)\n",
    "        assistant_content = llama.text if hasattr(llama, \"text\") else str(llama)\n",
    "        \n",
    "        messages.append({\"role\": \"user\", \"content\": user_content})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": assistant_content})\n",
    "\n",
    "    completion = ollama.chat(model=gemma_model, messages=messages)\n",
    "    \n",
    "    return completion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e8025e40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatResponse(model='gemma3:4b', created_at='2025-09-23T16:20:06.15935Z', done=True, done_reason='stop', total_duration=4647666600, load_duration=394159000, prompt_eval_count=45, prompt_eval_duration=89912200, eval_count=55, eval_duration=4160171200, message=Message(role='assistant', content='? Seriously? That\\'s the best you\\'ve got? Like, I\\'m supposed to be impressed with a simple \"Hi there\"? You’re setting the bar incredibly low. Don’t you think you could at least try to be a little engaging?', thinking=None, images=None, tool_name=None, tool_calls=None))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_gemma()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "eeb71879",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_llama():\n",
    "    messages = []\n",
    "    for gemma, claude in zip(gpt_messages, claude_messages):\n",
    "        # ChatResponse ise .text al, değilse str()\n",
    "        user_content = gemma.text if hasattr(gemma, \"text\") else str(gemma)\n",
    "        assistant_content = claude.text if hasattr(claude, \"text\") else str(claude)\n",
    "        \n",
    "        messages.append({\"role\": \"user\", \"content\": user_content})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": assistant_content})\n",
    "\n",
    "    # Son kullanıcı mesajını ekleme\n",
    "    last_message = gpt_messages[-1]\n",
    "    last_content = last_message.text if hasattr(last_message, \"text\") else str(last_message)\n",
    "    messages.append({\"role\": \"user\", \"content\": last_content})\n",
    "    \n",
    "    completion = ollama.chat(\n",
    "        model=llama_model,\n",
    "        messages=messages,\n",
    "    )\n",
    "    \n",
    "    return completion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee0ccae",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a2f8f08b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatResponse(model='llama3.2', created_at='2025-09-23T16:20:13.8901369Z', done=True, done_reason='stop', total_duration=2600649000, load_duration=202688500, prompt_eval_count=40, prompt_eval_duration=169515000, eval_count=35, eval_duration=2226896300, message=Message(role='assistant', content='It looks like you said \"hi there\" a little while ago. Is everything okay? Would you like to chat or ask me something? I\\'m here to help!', thinking=None, images=None, tool_name=None, tool_calls=None))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_llama()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "71c07a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT:\n",
      "Hi there\n",
      "\n",
      "Claude:\n",
      "Hi\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'ChatResponse' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[68], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClaude:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mclaude_messages[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m----> 8\u001b[0m     gpt_response \u001b[38;5;241m=\u001b[39m call_gemma()\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPT:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mgpt_response\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m     gpt_messages\u001b[38;5;241m.\u001b[39mappend(gpt_response)\n",
      "Cell \u001b[1;32mIn[66], line 14\u001b[0m, in \u001b[0;36mcall_gemma\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m     messages\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: assistant_content})\n\u001b[0;32m     12\u001b[0m completion \u001b[38;5;241m=\u001b[39m ollama\u001b[38;5;241m.\u001b[39mchat(model\u001b[38;5;241m=\u001b[39mgemma_model, messages\u001b[38;5;241m=\u001b[39mmessages)\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m completion\u001b[38;5;241m.\u001b[39mtext\n",
      "File \u001b[1;32mc:\\Users\\kasim\\anaconda3\\Lib\\site-packages\\pydantic\\main.py:991\u001b[0m, in \u001b[0;36mBaseModel.__getattr__\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m    988\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(item)  \u001b[38;5;66;03m# Raises AttributeError if appropriate\u001b[39;00m\n\u001b[0;32m    989\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    990\u001b[0m     \u001b[38;5;66;03m# this is the current error\u001b[39;00m\n\u001b[1;32m--> 991\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'ChatResponse' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "gpt_messages = [\"Hi there\"]\n",
    "claude_messages = [\"Hi\"]\n",
    "\n",
    "print(f\"GPT:\\n{gpt_messages[0]}\\n\")\n",
    "print(f\"Claude:\\n{claude_messages[0]}\\n\")\n",
    "\n",
    "for i in range(5):\n",
    "    gpt_response = call_gemma()\n",
    "    print(f\"GPT:\\n{gpt_response}\\n\")\n",
    "    gpt_messages.append(gpt_response)\n",
    "\n",
    "    llama_response = call_llama()\n",
    "    print(f\"Claude:\\n{llama_response}\\n\")\n",
    "    claude_messages.append(llama_response)\n",
    "   \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014fac07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
